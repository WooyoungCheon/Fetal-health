# -*- coding: utf-8 -*-
"""XGBoost

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10nuPDPi9_GBT0ZX11DEVyJKDud7Cmxug

# **<개요>**
Decision tree, Random forest, K-nearest neighbors, Gaussian naive Bayes, Adaboost, Gradient boost, XGBoost, Support vector machine 모델을 비교하는 코드입니다. **1. 2.**를 먼저 실행하고 원하는 모델을 실행시키면 됩니다. 각 모델 안에는 모델 설계와 학습, 테스트 과정이 함께 들어가 있습니다.
"""

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn import metrics
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import SMOTE
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree
from sklearn.ensemble import RandomForestClassifier,VotingClassifier,GradientBoostingClassifier,AdaBoostClassifier
from sklearn.svm import SVC
from mlxtend.classifier import StackingClassifier
from sklearn import model_selection
from sklearn.preprocessing import LabelEncoder
from xgboost.sklearn import XGBClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report
sns.set(color_codes=True) # adds a nice background to the graphs
# %matplotlib inline

"""# **1. Data loading, analysis, visualization**"""

df = pd.read_csv('/content/fetal_health.csv')
df.head()

df.shape
df.info()

df.isnull().sum()

df.head(10).style.background_gradient(cmap="RdYlBu")

#sns.pairplot(df) #시간 오래 걸려서 주석처리 했어요(데이터 확인용이라 굳이 필요한가 싶기도..)

df.describe().T

df.skew()

"""Let's check if there is any duplicates in the dataset."""

df[df.duplicated()]

df_dup = df.drop_duplicates(subset = None , keep = 'first', inplace = False)
df_dup.shape

Target = df["fetal_health"]

corr = df.corr()
mask = np.zeros_like(corr)
mask[np.triu_indices_from(mask)] = True
with sns.axes_style("white"):
    f, ax = plt.subplots(figsize=(15, 15))
    ax = sns.heatmap(corr,mask=mask,square=True,linewidths=2.5,cmap="viridis",annot=True)

"""There is strong correlation between baseline value and histogram mode , histogram median and histogram mean.

Histogram number of peaks and histogram width is also having good correlation.
"""

sns.countplot(Target)
plt.show()

print("Count of type 1.0 fetal health in the dataset ",len(df.loc[df["fetal_health"]==1.0]))
print("Count of type 2.0 fetal health in the dataset ",len(df.loc[df["fetal_health"]==2.0]))
print("Count of type 3.0 fetal health in the dataset ",len(df.loc[df["fetal_health"]==3.0]))

for item in ['histogram_width', 'histogram_median', 'histogram_variance', 'histogram_mode', 'histogram_number_of_peaks', 'histogram_number_of_zeroes','histogram_tendency','histogram_mean','histogram_max','histogram_min','abnormal_short_term_variability']:
  df_dup = df_dup.drop(item, axis = 1)

updated_cols = list(df.columns)
for column in updated_cols:
    print(column," : ", len(df.loc[df[column]<0]))

X = df_dup.iloc[:,:-1]
y = df_dup.iloc[:,-1]
print(X)

y.value_counts()

scale = StandardScaler()
X = scale.fit_transform(X)
X = pd.DataFrame(X,columns=df_dup.iloc[:,:-1].columns)

"""# **2. 라벨 수 불균형 해소하기**"""

from imblearn.over_sampling import RandomOverSampler
ROS = RandomOverSampler(random_state=42)
X_ros, y_ros = ROS.fit_resample(X,y)
from collections import Counter
print('Resampled dataset shape %s' % Counter(y_ros))

import statsmodels.api as sm
X = sm.add_constant(X)
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 10, test_size = 0.2)

print('X_train', X_train.shape)
print('y_train', y_train.shape)

print('X_test', X_test.shape)
print('y_test', y_test.shape)

print("{0:0.2f}% data is in training set".format((len(X_train)/len(df.index)) * 100))
print("{0:0.2f}% data is in test set".format((len(X_test)/len(y.index)) * 100))

def get_train_report(model):

    train_pred = model.predict(X_train)
    return(classification_report(y_train, train_pred))

def get_test_report(model):
    test_pred = model.predict(X_test)
    return(classification_report(y_test, test_pred))

"""# **Decision Tree Classifier**"""

decision_tree_classification = DecisionTreeClassifier(criterion = 'entropy', random_state = 10)
decision_tree = decision_tree_classification.fit(X_train, y_train)

train_report = get_train_report(decision_tree)
print(train_report)
test_report = get_test_report(decision_tree)
print(test_report)

dt_model = DecisionTreeClassifier(criterion = 'gini',
                                  max_depth = 5,
                                  min_samples_split = 4,
                                  max_leaf_nodes = 6,
                                  random_state = 10)

# fit the model using fit() on train data
decision_tree = dt_model.fit(X_train, y_train)
train_report = get_train_report(decision_tree)

# print the performance measures
print('Train data:\n', train_report)
test_report = get_test_report(decision_tree)

# print the performance measures
print('Test data:\n', test_report)

"""# **Random Forest**"""

rf_classification = RandomForestClassifier(n_estimators = 10, random_state = 10)

# use fit() to fit the model on the train set
rf_model = rf_classification.fit(X_train, y_train)

train_report = get_train_report(rf_model)
print(train_report)
test_report = get_test_report(rf_model)
print(test_report)

important_features = pd.DataFrame({'Features': X_train.columns,
                                   'Importance': rf_model.feature_importances_})

# sort the dataframe in the descending order according to the feature importance
important_features = important_features.sort_values('Importance', ascending = False)

# create a barplot to visualize the features based on their importance
sns.barplot(x = 'Importance', y = 'Features', data = important_features)

# add plot and axes labels
# set text size using 'fontsize'
plt.title('Feature Importance', fontsize = 15)
plt.xlabel('Importance', fontsize = 15)
plt.ylabel('Features', fontsize = 15)

# display the plot
plt.show()

"""From the above bar plot, we can see that short term variability is the most important feature in the dataset.

# **K Nearest Neighbors**
"""

from sklearn.metrics import confusion_matrix,roc_curve
knn_classification = KNeighborsClassifier(n_neighbors = 3)

# fit the model using fit() on train data
knn_model = knn_classification.fit(X_train, y_train)

train_report = get_train_report(knn_model)
print(train_report)
test_report = get_test_report(knn_model)
print(test_report)

tuned_paramaters = {'n_neighbors': np.arange(1, 25, 2),
                   'metric': ['hamming','euclidean','manhattan','Chebyshev']}

# instantiate the 'KNeighborsClassifier'
knn_classification = KNeighborsClassifier()

knn_grid = GridSearchCV(estimator = knn_classification,
                        param_grid = tuned_paramaters,
                        cv = 5,
                        scoring = 'accuracy')

# fit the model on X_train and y_train using fit()
knn_grid.fit(X_train, y_train)

# get the best parameters
print('Best parameters for KNN Classifier: ', knn_grid.best_params_, '\n')

"""Best parameters for KNN Classifier:  {'metric': 'manhattan', 'n_neighbors': 7}"""

from sklearn.model_selection import cross_val_score
error_rate = []

# use for loop to build a knn model for each K
for i in np.arange(1,25,2):

    # setup a knn classifier with k neighbors
    # use the 'euclidean' metric
    knn = KNeighborsClassifier(i, metric = 'euclidean')

    # fit the model using 'cross_val_score'
    # pass the knn model as 'estimator'
    # use 5-fold cross validation
    score = cross_val_score(knn, X_train, y_train, cv = 5)

    # calculate the mean score
    score = score.mean()

    # compute error rate
    error_rate.append(1 - score)

# plot the error_rate for different values of K
plt.plot(range(1,25,2), error_rate)

# add plot and axes labels
# set text size using 'fontsize'
plt.title('Error Rate', fontsize = 15)
plt.xlabel('K', fontsize = 15)
plt.ylabel('Error Rate', fontsize = 15)

# set the x-axis labels
plt.xticks(np.arange(1, 25, step = 2))

# plot a vertical line across the minimum error rate
plt.axvline(x = 7, color = 'red')

# display the plot
plt.show()

"""We can see that the optimal value of K = 7 obtained from the GridSearchCV results in a lowest error rate."""

train_report = get_train_report(knn_grid)
print(train_report)
test_report = get_test_report(knn_grid)
print(test_report)

"""# **Gaussian Naive Bayes**"""

gnb = GaussianNB()

# fit the model using fit() on train data
gnb_model = gnb.fit(X_train, y_train)

test_report = get_test_report(gnb_model)
print(test_report)

"""# **Adaboost Classifier**"""

ada_model = AdaBoostClassifier(n_estimators = 40, random_state = 10)
ada_model.fit(X_train, y_train)

test_report = get_test_report(ada_model)
print(test_report)

"""# **Gradient Boosting Classifier**"""

gboost_model = GradientBoostingClassifier(n_estimators = 150, max_depth = 10, random_state = 10)
gboost_model.fit(X_train, y_train)

test_report = get_test_report(gboost_model)
print(test_report)

"""# **XGBoost Classifier**"""

le = LabelEncoder()
y_train = le.fit_transform(y_train)
y_test = le.fit_transform(y_test)

xgb_model = XGBClassifier(max_depth = 10, gamma = 1)
xgb_model.fit(X_train, y_train)

from sklearn.metrics import accuracy_score

# 모델의 정확도 계산
xgb_train_accuracy = accuracy_score(y_train, xgb_model.predict(X_train))
xgb_test_accuracy = accuracy_score(y_test, xgb_model.predict(X_test))

# 과적합 정도 평가
xgb_overfitting_degree = xgb_train_accuracy - xgb_test_accuracy

print("Train Accuracy:", xgb_train_accuracy)
print("Test Accuracy:", xgb_test_accuracy)
print("Overfitting Degree:", xgb_overfitting_degree)

parameters = {"loss": ["deviance"],
              "learning_rate": [0.05, 0.075, 0.1, 0.25, 0.5, 0.75, 1],
              "n_estimators": [200, 350, 500, 750],
              "max_depth": [3, 6, 8]
              }

GridSearchCV_gbcl = GridSearchCV(estimator=XGBClassifier(),
                                param_grid=parameters,
                                cv=2,
                                verbose=1,
                                n_jobs=3,
                                scoring="accuracy",
                                return_train_score=True
                                )

GridSearchCV_gbcl.fit(X_train, y_train);

best_parameters = GridSearchCV_gbcl.best_params_
print(f"Best parameters for the model:\n{best_parameters}")

"""**Testing**"""

xgb = XGBClassifier(learning_rate=0.05, loss="log_loss", max_depth=3, n_estimators=200)

xgb_mod = xgb.fit(X_train, y_train)
pred_xgb = xgb_mod.predict(X_test)

score_xgb_train = xgb_mod.score(X_train, y_train)
score_xgb_test = xgb_mod.score(X_test, y_test)

train_report = get_train_report(xgb_mod)
print(train_report)
test_report = get_test_report(xgb_mod)
print(test_report)

important_features = pd.DataFrame({'Features': X_train.columns, 'Importance': xgb_model.feature_importances_})
important_features = important_features.sort_values('Importance', ascending = False)

sns.barplot(x = 'Importance', y = 'Features', data = important_features)
plt.title('Feature Importance', fontsize = 15)
plt.xlabel('Importance', fontsize = 15)
plt.ylabel('Features', fontsize = 15)
plt.show()

plt.subplots(figsize=(12,8))
cf_matrix = confusion_matrix(y_test, pred_xgb)
sns.heatmap(cf_matrix/np.sum(cf_matrix), cmap='viridis',annot = True, annot_kws = {'size':20})
plt.show()

"""# **Support Vector Machine**"""

svc_model = SVC(kernel='poly',probability=True)
svc_model.fit(X_train,y_train)

test_report = get_test_report(svc_model)
print(test_report)

"""Voting Classifier"""

clf1 = KNeighborsClassifier(n_neighbors = 7 , weights = 'distance', metric='manhattan' )
clf2 = GradientBoostingClassifier(n_estimators = 150,max_depth = 10,random_state=1)

votingclf = VotingClassifier(estimators=[('knn',clf1),('grb', clf2)],voting='hard')
votingclf = votingclf.fit(X_train,y_train)

test_report = get_test_report(votingclf)
print(test_report)

"""We tried different algorithms for this dataset among them the boosting based algorithms i.e, XG Boost and Gradient boosting algorithms are performing best for the dataset with an accuracy of 94% on the test dataset and f1 score for XGBoost are 0.96 , 0.85 and 0.87 respectively for three different classes followed by Decision Tree Classifier without hypertuning it with an accuracy of 93% on the test data.

*PLEASE UPVOTE IF YOU LIKE THE ANALYSIS*
"""